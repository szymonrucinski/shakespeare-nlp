{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13020bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few imports useful to any data scientist.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "logger = logging.getLogger()\n",
    "pd.set_option('display.max_columns', 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7c028",
   "metadata": {},
   "source": [
    "![Shakespeare](shakespeare.jpg \"William Shakespeare\")\n",
    "\n",
    "<h3>William Shakespeare is a little-known playwright from the 16th and 17th centuries.\n",
    "In the following few exercises, we have a look at lines from Shakespeare's plays\n",
    "and try to implement a simple text generator to write just like Shakespeare.</h3>\n",
    "\n",
    "Note: You are free to solve exercises in any manner you wish. There is no specific requirement to use the code already provided in the notebook cells, but the code is there in case you choose to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1521ee",
   "metadata": {},
   "source": [
    "A dataset has been kindly provided to us by the fine folks at Kaggle. This dataset contains every line from every major Shakespeare play, along with information about the current actor speaking the line, the play this line originated from, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddaa5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is provided as both a pandas dataframe or a tuple list. Again, if you prefer using another format, you are free to do so.\n",
    "dataset = pd.read_csv(\"Shakespeare_data.csv\")\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53540f",
   "metadata": {},
   "source": [
    "Let's view the first ten elements of the dataset to get an idea about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2f968",
   "metadata": {},
   "source": [
    "<h4>EXERCISE 1: Drop from our dataset all lines which are not dialogue.</h4>\n",
    "\n",
    "Example input dataset (first six rows):\\\n",
    "(1, 'Henry IV', nan, nan, nan, 'ACT I')\\\n",
    "(2, 'Henry IV', nan, nan, nan, 'SCENE I. London. The palace.')\\\n",
    "(3, 'Henry IV', nan, nan, nan, 'Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others')\\\n",
    "(4, 'Henry IV', 1.0, '1.1.1', 'KING HENRY IV', 'So shaken as we are, so wan with care,')\\\n",
    "(5, 'Henry IV', 1.0, '1.1.2', 'KING HENRY IV', 'Find we a time for frighted peace to pant,')\\\n",
    "(6, 'Henry IV', 1.0, '1.1.3', 'KING HENRY IV', 'And breathe short-winded accents of new broils')\n",
    "\n",
    "Example output dataset (first six rows):\\\n",
    "(4, 'Henry IV', 1.0, '1.1.1', 'KING HENRY IV', 'So shaken as we are, so wan with care,')\\\n",
    "(5, 'Henry IV', 1.0, '1.1.2', 'KING HENRY IV', 'Find we a time for frighted peace to pant,')\\\n",
    "(6, 'Henry IV', 1.0, '1.1.3', 'KING HENRY IV', 'And breathe short-winded accents of new broils')\\\n",
    "(7, 'Henry IV', 1.0, '1.1.4', 'KING HENRY IV', 'To be commenced in strands afar remote.')\\\n",
    "(8, 'Henry IV', 1.0, '1.1.5', 'KING HENRY IV', 'No more the thirsty entrance of this soil')\\\n",
    "(9, 'Henry IV', 1.0, '1.1.6', 'KING HENRY IV', \"Shall daub her lips with her own children's blood,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ba2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset[~dataset.Player.isnull()]\n",
    "print(\"Number of lines: \", len(dataset))\n",
    "print(\"Number of lines: \", len(dialogues))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0e59d",
   "metadata": {},
   "source": [
    "<h4>EXERCISE 2: Group all dialogue lines for the entire dataset together into a single large string (join lines by a single whitespace).</h4>\t\n",
    "\n",
    "Example input dataset (first six rows):\\\n",
    "(4, 'Henry IV', 1.0, '1.1.1', 'KING HENRY IV', 'So shaken as we are, so wan with care,')\\\n",
    "(5, 'Henry IV', 1.0, '1.1.2', 'KING HENRY IV', 'Find we a time for frighted peace to pant,')\\\n",
    "(6, 'Henry IV', 1.0, '1.1.3', 'KING HENRY IV', 'And breathe short-winded accents of new broils')\\\n",
    "(7, 'Henry IV', 1.0, '1.1.4', 'KING HENRY IV', 'To be commenced in strands afar remote.')\\\n",
    "(8, 'Henry IV', 1.0, '1.1.5', 'KING HENRY IV', 'No more the thirsty entrance of this soil')\\\n",
    "(9, 'Henry IV', 1.0, '1.1.6', 'KING HENRY IV', \"Shall daub her lips with her own children's blood,\")\n",
    "\n",
    "Example output string:\\\n",
    "So shaken as we are, so wan with care, Find we a time for frighted peace to pant, And breathe short-winded accents of new broils To be commenced in strands afar remote. No more the thirsty entrance of this soil Shall daub her lips with her own children's blood, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_string_dialogue = \" \".join(dialogues.PlayerLine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba101f94",
   "metadata": {},
   "source": [
    "<h4>EXERCISE 3: Implement a simple tokenization: disregard any non-alpha characters except\n",
    ".!?:', which you should treat as single tokens. All other words should be regarded as\n",
    "single tokens. Convert all word tokens to lowercase.</h4>\n",
    "\n",
    "Example input: \"So shaken as we are, so wan with care, Find we a time\"\n",
    "\n",
    "Example output: ['so', 'shaken', 'as', 'we', 'are', ',', 'so', 'wan', 'with', 'care', 'find','a','time' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a mistake in output: \"so\" is repeated twice.\n",
    "punctuation = \".!?:',\"\n",
    "def tokenize(text, order_matters=True):\n",
    "    text = text.lower()\n",
    "    #w+ matches one or more word characters (same as [a-zA-Z0-9_]+\n",
    "    word_pattern = rF\"([a-z0-9]+|[{punctuation}])\"\n",
    "    matches = re.findall(word_pattern, text)\n",
    "    return matches\n",
    "\n",
    "    if order_matters:\n",
    "        return [x for i, x in enumerate(matches) if x not in matches[:i]]\n",
    "    else:\n",
    "        return list(set(matches))\n",
    "\n",
    "input = \"So shaken as we are, so wan with care, Find we a time\"\n",
    "start_time = time.perf_counter()\n",
    "output = tokenize(input)\n",
    "end_time = time.perf_counter()\n",
    "print(output)\n",
    "\n",
    "assert  output == ['so', 'shaken', 'as', 'we', 'are', ',', 'so','wan', 'with', 'care', ',', 'find', 'we','a','time' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65bed4",
   "metadata": {},
   "source": [
    "<h4>EXERCISE 4: List the 50 most common tokens and their occurrence amount.</h4>\n",
    "\n",
    "Example output:\\\n",
    "{',': 95042, '.': 33787, 'the': 26027, \"'\": 24099, 'and': 23443, 'i': 21772, 'to': 18800, 'of': 15446, 'you': 13579, ':': 13507, 'a': 13481, 'my': 11875, 'that': 10843, 'in': 10365, '?': 10039, 'is': 8997, '!': 8855, 'not': 8234, 'it': 7492, 'me': 7489, 'for': 7433, 's': 7121, 'with': 6957, 'be': 6697, 'he': 6521, 'your': 6507, 'this': 6446, 'his': 6347, 'but': 5985, 'have': 5754, 'as': 5500, 'thou': 5273, 'd': 5062, 'him': 4960, 'will': 4864 ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1dee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenize(one_string_dialogue)\n",
    "def count_tokens(text):\n",
    "    keys = set(output)\n",
    "    most_common_tokens_dict = {}\n",
    "    for key in keys:\n",
    "        most_common_tokens_dict[key] = output.count(key)\n",
    "        # remove the key from the list\n",
    "        text = list(filter((key).__ne__, text))\n",
    "    return most_common_tokens_dict\n",
    "\n",
    "top_50_words = None\n",
    "\n",
    "if os.path.exists(\"top_50_words.pickle\") == False:\n",
    "    word_count = count_tokens(output)\n",
    "    #count the time it takes to run the function\n",
    "    top_50_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "    # save the top 50 words to a pickle file\n",
    "    with open(\"top_50_words.pickle\", \"wb\") as outfile:\n",
    "    # \"wb\" argument opens the file in binary mode\n",
    "        pickle.dump(top_50_words, outfile)\n",
    "else:\n",
    "    with open(\"top_50_words.pickle\", \"rb\") as infile:\n",
    "        # \"rb\" argument opens the file in binary mode\n",
    "        top_50_words = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4fa2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(x=[x[0] for x in top_50_words], y=[x[1] for x in top_50_words])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e30e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split list into chunks of size n\n",
    "break\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "chunks = list(chunks(output, 520000))\n",
    "\n",
    "from multiprocessing import Pool\n",
    "results = []\n",
    "with Pool(2) as pool:\n",
    "        results = pool.map(tokenize, chunks)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c23437",
   "metadata": {},
   "source": [
    "<h4>EXERCISE 5: For each word, instantiate a dictionary of words that follow it in the corpus,\n",
    "as well as the number of occurrences of follow words.</h4>\n",
    "\n",
    "Input: ['we', 'did', 'not', 'think', 'we', 'did', 'bad', '.']\n",
    "\n",
    "Output: {\n",
    "    'we': {'did': 2},\n",
    "    'did': {'not': 1, 'bad': 1},\n",
    "    'not': {'think': 1},\n",
    "    'think': {'we': 1},\n",
    "    'bad': {'.': 1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class ZeroDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return 0\n",
    "\n",
    "def build_ngram_dict(tokens, n=1):\n",
    "    # create a defaultdict to store ngram counts\n",
    "    ngram_dict = defaultdict(ZeroDict)\n",
    "    \n",
    "    # loop over tokens\n",
    "    for i in range(len(tokens)-n):\n",
    "        # if ngrams are unigrams\n",
    "        if n == 1:\n",
    "            # increment the count for the ngram (key=tokens[i], value=tokens[i+n])\n",
    "            ngram_dict[tokens[i]][tokens[i + n]] += 1        \n",
    "        # if ngrams are not unigrams\n",
    "        else:\n",
    "            # increment the count for the ngram (key=tokens[i:i+n], value=tokens[i+n])\n",
    "            ngram_dict[str(tokens[i:i + n])][tokens[i + n]] += 1\n",
    "    \n",
    "    # return the ngram dictionary\n",
    "    return ngram_dict\n",
    "\n",
    "if os.path.exists(\"unigram_dict.pickle\") == False:\n",
    "    ngram_dict = build_ngram_dict(output, n=1)\n",
    "    with open(\"unigram_dict.pickle\", \"wb\") as outfile:\n",
    "        pickle.dump(ngram_dict, outfile)\n",
    "else:\n",
    "    with open(\"unigram_dict.pickle\", \"rb\") as infile:\n",
    "        ngram_dict = pickle.load(infile)\n",
    "\n",
    "display(ngram_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea7e17",
   "metadata": {},
   "source": [
    "<h4>EXERCISE 6: Starting from a single word, \"i\", generate text by sampling possible subsequent words given the word statistics you previously built, up to 1000 tokens.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "\n",
    "    def __init__(self,n=1):\n",
    "        self.n = n\n",
    "\n",
    "    def fit(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.ngram_dict = build_ngram_dict(tokens, n=self.n)\n",
    "        \n",
    "    def generate(self, seq_length=20, seq=[\"i\"]):\n",
    "        for i in range(seq_length):\n",
    "            # Get the last word in the sequence.\n",
    "            last_word = seq[-1]\n",
    "            # Get the dictionary of words that follow the last word.\n",
    "            next_words = self.ngram_dict[last_word]\n",
    "\n",
    "            # Get the most common word that follows the last word.\n",
    "            most_common_word = max(next_words, key=next_words.get)\n",
    "            \n",
    "            # Add the most common word to the sequence.\n",
    "            seq.append(most_common_word)\n",
    "\n",
    "        return \" \".join(seq)\n",
    "\n",
    "unigram_model = UnigramModel(n=1)\n",
    "unigram_model.fit(tokens=output)\n",
    "print(unigram_model.generate(seq=[\"i\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2962fb1b",
   "metadata": {},
   "source": [
    "### Remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc434d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWordsEng = stopwords.words(\"english\")\n",
    "# stopWordsEng.remove(\"i\")\n",
    "\n",
    "# output_without_sw = [word for word in output if not word in stopWordsEng]\n",
    "output_without_sw = [word for word in output if not word in list(punctuation)]\n",
    "output_without_sw = [word for word in output_without_sw if not word in [\"and\"]]\n",
    "\n",
    "unigram_model = UnigramModel(n=1)\n",
    "unigram_model.fit(tokens=output_without_sw)\n",
    "print(unigram_model.generate(seq=[\"i\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc4cc49",
   "metadata": {},
   "source": [
    "<h4>EXERCISE 7: What kind of avenues would you think of to improve this text generation?</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGramModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.single_gram_dict = build_ngram_dict(tokens, n=1)\n",
    "        self.double_gram_dict = build_ngram_dict(tokens, n=2)\n",
    "        \n",
    "    def predict_token(self,words):\n",
    "        \n",
    "        if len(words) == 1:\n",
    "            next_words = self.single_gram_dict[words[-1]]\n",
    "            return max(next_words, key=next_words.get)\n",
    "\n",
    "    \n",
    "        elif len(words) > 1:\n",
    "            # next_words = self.single_gram_dict[words[0]]\n",
    "            # one_gram_prediction = max(next_words, key=next_words.get)\n",
    "            # print(max(next_words.values()))\n",
    "            key = str(words)\n",
    "            next_words = self.double_gram_dict[key]\n",
    "            try:\n",
    "                return max(next_words, key=next_words.get)\n",
    "            except:\n",
    "                next_words = self.single_gram_dict[words[-1]]\n",
    "                return max(next_words, key=next_words.get)\n",
    "        \n",
    "    def generate(self,seq,seq_length=20):\n",
    "        for i in range(seq_length):\n",
    "            if i == 0:\n",
    "               seq.append(self.predict_token(seq[0]))\n",
    "            elif i >= 1:\n",
    "                seq.append(self.predict_token(seq[-2:]))                \n",
    "\n",
    "        return \" \".join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensaemble_model = TwoGramModel()\n",
    "ensaemble_model.fit(tokens=output)\n",
    "print(ensaemble_model.generate(seq=[\"i\"], seq_length=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009268de",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoGramModel = TwoGramModel()\n",
    "twoGramModel.fit(tokens=output)\n",
    "print(twoGramModel.generate(seq=[\"i\"], seq_length=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoGramModel = TwoGramModel()\n",
    "twoGramModel.fit(tokens=output_without_punct)\n",
    "print(twoGramModel.generate(seq=[\"i\"], seq_length=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shakespeare-nlp-AcEoqpRu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "41eafb823ffa09f41e3af8606e21dbb57d8bcf9cb12cc229d945b5e3d48bceec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
